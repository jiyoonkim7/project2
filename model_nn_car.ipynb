{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyoonkim7/project2/blob/main/model_nn_car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "C5W7dO_EyyLV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split,TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data using numpy\n",
        "data = np.loadtxt(\"/Lidar.samples\", dtype=np.double)"
      ],
      "metadata": {
        "id": "dWRTQr8hEPHi"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKc7MXoAyyLY",
        "outputId": "ebf19524-eae5-475c-f7ed-016c17036011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Saving the read out data in numpy arrays\n",
        "dataY = data[:, -1:] \n",
        "dataX = data[:,:-1]\n",
        "\n",
        "# Transforming numpy arrays to tensors\n",
        "dataX_tensor = torch.tensor(dataX)\n",
        "dataY_tensor = torch.tensor(dataY)\n",
        "\n",
        "# Combine input and output tensors into a tensordataset\n",
        "dataset = TensorDataset(dataX_tensor, dataY_tensor)\n",
        "\n",
        "# Define the size of your training and testing subsets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Manual seed for reproducible results\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], torch.Generator().manual_seed(42))\n",
        "\n",
        "# Use dataloader for batching and iterating\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "tgkJXGTR3kmx"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "5v5UUT1DyyLZ"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "class NeuralNetwork(nn.Module): #from torch import nn\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1hiddenlayer = nn.Sequential(\n",
        "            nn.Linear(16, 64), \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(32,1)\n",
        "       \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_1hiddenlayer(x)\n",
        "        return logits\n",
        "\n",
        "# Load neural network\n",
        "model = NeuralNetwork().double().to(device) #keep it float64 and move model to GPU \n",
        "\n",
        "# Choosing loss function and optimizer\n",
        "loss_fn = nn.MSELoss() \n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-1) # try Adam  or Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMbdIjO8yyLa",
        "outputId": "8e4caa5a-ed62-4b95-a5aa-f980ba4e508d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16])\n",
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "test_tensor = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).to('cuda').double()\n",
        "print(test_tensor.shape)\n",
        "print(test_tensor.dtype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U859R-ulyyLb"
      },
      "outputs": [],
      "source": [
        "model(test_tensor).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4uCFglCyyLb",
        "outputId": "b8949a7d-4be7-428e-f927-c12cffd10ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs:    1 | Loss: 126.2509399892\n",
            "Epochs:    2 | Loss: 77.9623798184\n",
            "Epochs:    3 | Loss: 71.4764260178\n",
            "Epochs:    4 | Loss: 67.1823460972\n",
            "Epochs:    5 | Loss: 64.6868476827\n",
            "Epochs:    6 | Loss: 63.8482754002\n",
            "Epochs:    7 | Loss: 62.3109187178\n",
            "Epochs:    8 | Loss: 61.7688086314\n",
            "Epochs:    9 | Loss: 60.1754210824\n",
            "Epochs:   10 | Loss: 57.6326739077\n",
            "Epochs:   11 | Loss: 57.1469437321\n",
            "Epochs:   12 | Loss: 57.1541345789\n",
            "Epochs:   13 | Loss: 55.8873729436\n",
            "Epochs:   14 | Loss: 55.2504056908\n",
            "Epochs:   15 | Loss: 55.0573333557\n",
            "Epochs:   16 | Loss: 53.9278258956\n",
            "Epochs:   17 | Loss: 53.2399212215\n",
            "Epochs:   18 | Loss: 53.4735071688\n",
            "Epochs:   19 | Loss: 51.7153018290\n",
            "Epochs:   20 | Loss: 51.7854926874\n",
            "Epochs:   21 | Loss: 51.5772079959\n",
            "Epochs:   22 | Loss: 51.5336735828\n",
            "Epochs:   23 | Loss: 50.4439467946\n",
            "Epochs:   24 | Loss: 51.1205672155\n",
            "Epochs:   25 | Loss: 50.5216196172\n",
            "Epochs:   26 | Loss: 50.0588870534\n",
            "Epochs:   27 | Loss: 49.4910800245\n",
            "Epochs:   28 | Loss: 48.4528039205\n",
            "Epochs:   29 | Loss: 49.6347604417\n",
            "Epochs:   30 | Loss: 48.3784797972\n",
            "Epochs:   31 | Loss: 48.3101990690\n",
            "Epochs:   32 | Loss: 48.6352358449\n",
            "Epochs:   33 | Loss: 48.2443441037\n",
            "Epochs:   34 | Loss: 46.7946599321\n",
            "Epochs:   35 | Loss: 47.6124071911\n",
            "Epochs:   36 | Loss: 47.9946665617\n",
            "Epochs:   37 | Loss: 47.7486330972\n",
            "Epochs:   38 | Loss: 46.8001462095\n",
            "Epochs:   39 | Loss: 46.7815781656\n",
            "Epochs:   40 | Loss: 46.8106144885\n",
            "Epochs:   41 | Loss: 46.3394923684\n",
            "Epochs:   42 | Loss: 46.5361939807\n",
            "Epochs:   43 | Loss: 46.4546513607\n",
            "Epochs:   44 | Loss: 45.3288442718\n",
            "Epochs:   45 | Loss: 47.1245924191\n",
            "Epochs:   46 | Loss: 46.3817143063\n",
            "Epochs:   47 | Loss: 45.3680180162\n",
            "Epochs:   48 | Loss: 46.1746319576\n",
            "Epochs:   49 | Loss: 45.3490057709\n",
            "Epochs:   50 | Loss: 46.2652898191\n",
            "Epochs:   51 | Loss: 44.8269430738\n",
            "Epochs:   52 | Loss: 45.4509405547\n",
            "Epochs:   53 | Loss: 44.3989468809\n",
            "Epochs:   54 | Loss: 45.1151697836\n",
            "Epochs:   55 | Loss: 44.3971241278\n",
            "Epochs:   56 | Loss: 45.4278999298\n",
            "Epochs:   57 | Loss: 44.7480088300\n",
            "Epochs:   58 | Loss: 44.6447240760\n",
            "Epochs:   59 | Loss: 45.0142996277\n",
            "Epochs:   60 | Loss: 44.4233952416\n",
            "Epochs:   61 | Loss: 44.2264947240\n",
            "Epochs:   62 | Loss: 44.1933485707\n",
            "Epochs:   63 | Loss: 44.6580043878\n",
            "Epochs:   64 | Loss: 44.8738685378\n",
            "Epochs:   65 | Loss: 43.5557019631\n",
            "Epochs:   66 | Loss: 43.7896135303\n",
            "Epochs:   67 | Loss: 43.9968701978\n",
            "Epochs:   68 | Loss: 44.1377459698\n",
            "Epochs:   69 | Loss: 43.4069652597\n",
            "Epochs:   70 | Loss: 43.7403044155\n",
            "Epochs:   71 | Loss: 43.3990341004\n",
            "Epochs:   72 | Loss: 43.0078361835\n",
            "Epochs:   73 | Loss: 43.9373157971\n",
            "Epochs:   74 | Loss: 43.7317830409\n",
            "Epochs:   75 | Loss: 43.4203638178\n",
            "Epochs:   76 | Loss: 43.2129822146\n",
            "Epochs:   77 | Loss: 42.9570842605\n",
            "Epochs:   78 | Loss: 42.3433163336\n",
            "Epochs:   79 | Loss: 42.7280321785\n",
            "Epochs:   80 | Loss: 43.1261243316\n",
            "Epochs:   81 | Loss: 43.0400567043\n",
            "Epochs:   82 | Loss: 41.8175756082\n",
            "Epochs:   83 | Loss: 43.3750298711\n",
            "Epochs:   84 | Loss: 43.5354116129\n",
            "Epochs:   85 | Loss: 42.2061948356\n",
            "Epochs:   86 | Loss: 42.7023904459\n",
            "Epochs:   87 | Loss: 42.9180252692\n",
            "Epochs:   88 | Loss: 42.8988353484\n",
            "Epochs:   89 | Loss: 41.7348505366\n",
            "Epochs:   90 | Loss: 42.3856211354\n",
            "Epochs:   91 | Loss: 42.0238394248\n",
            "Epochs:   92 | Loss: 42.7512746094\n",
            "Epochs:   93 | Loss: 41.9793630689\n",
            "Epochs:   94 | Loss: 41.3942906825\n",
            "Epochs:   95 | Loss: 42.2282761190\n",
            "Epochs:   96 | Loss: 42.2315562568\n",
            "Epochs:   97 | Loss: 41.4042335833\n",
            "Epochs:   98 | Loss: 42.0676994633\n",
            "Epochs:   99 | Loss: 41.0061600729\n",
            "Epochs:  100 | Loss: 41.8753265254\n",
            "Epochs:  101 | Loss: 43.1944211281\n",
            "Epochs:  102 | Loss: 41.6157063273\n",
            "Epochs:  103 | Loss: 42.6474361794\n",
            "Epochs:  104 | Loss: 42.3745621224\n",
            "Epochs:  105 | Loss: 42.0338041935\n",
            "Epochs:  106 | Loss: 41.5189308371\n",
            "Epochs:  107 | Loss: 40.7906013784\n",
            "Epochs:  108 | Loss: 41.5348937551\n",
            "Epochs:  109 | Loss: 41.4608153630\n",
            "Epochs:  110 | Loss: 41.3076536902\n",
            "Epochs:  111 | Loss: 41.6609242001\n",
            "Epochs:  112 | Loss: 42.3654805364\n",
            "Epochs:  113 | Loss: 41.6210174344\n",
            "Epochs:  114 | Loss: 41.5463814757\n",
            "Epochs:  115 | Loss: 40.4529633145\n",
            "Epochs:  116 | Loss: 40.8734969820\n",
            "Epochs:  117 | Loss: 40.1401261172\n",
            "Epochs:  118 | Loss: 41.5402120060\n",
            "Epochs:  119 | Loss: 41.8436652969\n",
            "Epochs:  120 | Loss: 41.2907666152\n",
            "Epochs:  121 | Loss: 41.3537872542\n",
            "Epochs:  122 | Loss: 41.4419304663\n",
            "Epochs:  123 | Loss: 40.6054803693\n",
            "Epochs:  124 | Loss: 41.0765418160\n",
            "Epochs:  125 | Loss: 41.3265683797\n",
            "Epochs:  126 | Loss: 40.8024285445\n",
            "Epochs:  127 | Loss: 42.0481897520\n",
            "Epochs:  128 | Loss: 41.0993881068\n",
            "Epochs:  129 | Loss: 40.2719176170\n",
            "Epochs:  130 | Loss: 39.9841666353\n",
            "Epochs:  131 | Loss: 40.7707336437\n",
            "Epochs:  132 | Loss: 39.7098077329\n",
            "Epochs:  133 | Loss: 40.6343134799\n",
            "Epochs:  134 | Loss: 40.0594004190\n",
            "Epochs:  135 | Loss: 41.0803381193\n",
            "Epochs:  136 | Loss: 40.0162931627\n",
            "Epochs:  137 | Loss: 40.4555155984\n",
            "Epochs:  138 | Loss: 40.5255018968\n",
            "Epochs:  139 | Loss: 40.9716758412\n",
            "Epochs:  140 | Loss: 40.2616159288\n",
            "Epochs:  141 | Loss: 40.2969697368\n",
            "Epochs:  142 | Loss: 40.6042740632\n",
            "Epochs:  143 | Loss: 40.8949064397\n",
            "Epochs:  144 | Loss: 40.6130647984\n",
            "Epochs:  145 | Loss: 40.2895773338\n",
            "Epochs:  146 | Loss: 40.4518000421\n",
            "Epochs:  147 | Loss: 40.5703334786\n",
            "Epochs:  148 | Loss: 40.1579606472\n",
            "Epochs:  149 | Loss: 40.1202084275\n",
            "Epochs:  150 | Loss: 39.9721908540\n",
            "Epochs:  151 | Loss: 40.2481569824\n",
            "Epochs:  152 | Loss: 40.1711482207\n",
            "Epochs:  153 | Loss: 39.8367138899\n",
            "Epochs:  154 | Loss: 39.6728586569\n",
            "Epochs:  155 | Loss: 40.6312746694\n",
            "Epochs:  156 | Loss: 39.2329375895\n",
            "Epochs:  157 | Loss: 39.6155916673\n",
            "Epochs:  158 | Loss: 39.1424703958\n",
            "Epochs:  159 | Loss: 38.7305730007\n",
            "Epochs:  160 | Loss: 39.3483539429\n",
            "Epochs:  161 | Loss: 39.2809825828\n",
            "Epochs:  162 | Loss: 39.3706547479\n",
            "Epochs:  163 | Loss: 39.4978989391\n",
            "Epochs:  164 | Loss: 39.2986525507\n",
            "Epochs:  165 | Loss: 40.3322401278\n",
            "Epochs:  166 | Loss: 39.4752983347\n",
            "Epochs:  167 | Loss: 38.9432077412\n",
            "Epochs:  168 | Loss: 40.2031105279\n",
            "Epochs:  169 | Loss: 39.5439862307\n",
            "Epochs:  170 | Loss: 38.8132979423\n",
            "Epochs:  171 | Loss: 39.0255404915\n",
            "Epochs:  172 | Loss: 39.8356781190\n",
            "Epochs:  173 | Loss: 39.1288004279\n",
            "Epochs:  174 | Loss: 38.7065553419\n",
            "Epochs:  175 | Loss: 39.3385242194\n",
            "Epochs:  176 | Loss: 39.7061215626\n",
            "Epochs:  177 | Loss: 39.6722664362\n",
            "Epochs:  178 | Loss: 39.3631323588\n",
            "Epochs:  179 | Loss: 39.3118367891\n",
            "Epochs:  180 | Loss: 38.9786687062\n",
            "Epochs:  181 | Loss: 40.5423420473\n",
            "Epochs:  182 | Loss: 39.3199692339\n",
            "Epochs:  183 | Loss: 39.1136118864\n",
            "Epochs:  184 | Loss: 39.1547474525\n",
            "Epochs:  185 | Loss: 39.2327762161\n",
            "Epochs:  186 | Loss: 38.7305200885\n",
            "Epochs:  187 | Loss: 38.8490246999\n",
            "Epochs:  188 | Loss: 39.0716789278\n",
            "Epochs:  189 | Loss: 38.1558089944\n",
            "Epochs:  190 | Loss: 38.6974687994\n",
            "Epochs:  191 | Loss: 39.1754211086\n",
            "Epochs:  192 | Loss: 38.4333938520\n",
            "Epochs:  193 | Loss: 38.8974677416\n",
            "Epochs:  194 | Loss: 38.3007442271\n",
            "Epochs:  195 | Loss: 39.2464617466\n",
            "Epochs:  196 | Loss: 38.8103945537\n",
            "Epochs:  197 | Loss: 38.5399222620\n",
            "Epochs:  198 | Loss: 39.4142841087\n",
            "Epochs:  199 | Loss: 38.5966440164\n",
            "Epochs:  200 | Loss: 38.7809749320\n",
            "Epochs:  201 | Loss: 38.9023589609\n",
            "Epochs:  202 | Loss: 38.2195435038\n",
            "Epochs:  203 | Loss: 39.5760065601\n",
            "Epochs:  204 | Loss: 38.2488663048\n",
            "Epochs:  205 | Loss: 39.0073294965\n",
            "Epochs:  206 | Loss: 38.5660815046\n",
            "Epochs:  207 | Loss: 38.2385084666\n",
            "Epochs:  208 | Loss: 38.7664742647\n",
            "Epochs:  209 | Loss: 38.7434765377\n",
            "Epochs:  210 | Loss: 38.7379816074\n",
            "Epochs:  211 | Loss: 38.0159667066\n",
            "Epochs:  212 | Loss: 37.4103853914\n",
            "Epochs:  213 | Loss: 38.0684847833\n",
            "Epochs:  214 | Loss: 38.5279409669\n",
            "Epochs:  215 | Loss: 37.5082322953\n",
            "Epochs:  216 | Loss: 38.2280960016\n",
            "Epochs:  217 | Loss: 37.8488789519\n",
            "Epochs:  218 | Loss: 37.2961356388\n",
            "Epochs:  219 | Loss: 38.5063583807\n",
            "Epochs:  220 | Loss: 38.5769522449\n",
            "Epochs:  221 | Loss: 37.6720421449\n",
            "Epochs:  222 | Loss: 37.3647589827\n",
            "Epochs:  223 | Loss: 38.4137107735\n",
            "Epochs:  224 | Loss: 38.6268570139\n",
            "Epochs:  225 | Loss: 38.5076835993\n",
            "Epochs:  226 | Loss: 37.6714979118\n",
            "Epochs:  227 | Loss: 38.4116494355\n",
            "Epochs:  228 | Loss: 37.5212417789\n",
            "Epochs:  229 | Loss: 36.7630356924\n",
            "Epochs:  230 | Loss: 38.1949899312\n",
            "Epochs:  231 | Loss: 38.3578372411\n",
            "Epochs:  232 | Loss: 37.1045399694\n",
            "Epochs:  233 | Loss: 37.8777384240\n",
            "Epochs:  234 | Loss: 38.0074036093\n",
            "Epochs:  235 | Loss: 37.8120319608\n",
            "Epochs:  236 | Loss: 37.6533340091\n",
            "Epochs:  237 | Loss: 37.1541996282\n",
            "Epochs:  238 | Loss: 37.4164796769\n",
            "Epochs:  239 | Loss: 37.2778081516\n",
            "Epochs:  240 | Loss: 36.1639514385\n",
            "Epochs:  241 | Loss: 36.9994123264\n",
            "Epochs:  242 | Loss: 37.3435679804\n",
            "Epochs:  243 | Loss: 37.3180729424\n",
            "Epochs:  244 | Loss: 36.7873421570\n",
            "Epochs:  245 | Loss: 36.0070806953\n",
            "Epochs:  246 | Loss: 37.5555690785\n",
            "Epochs:  247 | Loss: 37.6618327632\n",
            "Epochs:  248 | Loss: 37.3029078949\n",
            "Epochs:  249 | Loss: 37.1707608909\n",
            "Epochs:  250 | Loss: 37.1761598576\n",
            "Epochs:  251 | Loss: 37.0517058938\n",
            "Epochs:  252 | Loss: 36.9655054147\n",
            "Epochs:  253 | Loss: 37.2402643154\n",
            "Epochs:  254 | Loss: 36.9145328061\n",
            "Epochs:  255 | Loss: 36.7071466253\n",
            "Epochs:  256 | Loss: 37.1364187646\n",
            "Epochs:  257 | Loss: 37.1012869056\n",
            "Epochs:  258 | Loss: 37.6408253725\n",
            "Epochs:  259 | Loss: 36.6408642770\n",
            "Epochs:  260 | Loss: 36.1854929008\n",
            "Epochs:  261 | Loss: 36.7733341469\n",
            "Epochs:  262 | Loss: 37.1415636071\n",
            "Epochs:  263 | Loss: 36.3619596004\n",
            "Epochs:  264 | Loss: 36.8153082205\n",
            "Epochs:  265 | Loss: 36.5146159919\n",
            "Epochs:  266 | Loss: 37.0727474747\n",
            "Epochs:  267 | Loss: 36.8595728818\n",
            "Epochs:  268 | Loss: 37.1200133655\n",
            "Epochs:  269 | Loss: 36.7231673968\n",
            "Epochs:  270 | Loss: 36.1902070320\n",
            "Epochs:  271 | Loss: 35.9155271041\n",
            "Epochs:  272 | Loss: 36.0665549207\n",
            "Epochs:  273 | Loss: 36.3432026345\n",
            "Epochs:  274 | Loss: 35.7250652035\n",
            "Epochs:  275 | Loss: 36.2048600682\n",
            "Epochs:  276 | Loss: 36.2240174005\n",
            "Epochs:  277 | Loss: 36.2264465496\n",
            "Epochs:  278 | Loss: 36.6315514567\n",
            "Epochs:  279 | Loss: 36.8532602742\n",
            "Epochs:  280 | Loss: 36.3028833294\n",
            "Epochs:  281 | Loss: 36.6579807925\n",
            "Epochs:  282 | Loss: 35.9486725579\n",
            "Epochs:  283 | Loss: 35.8824615200\n",
            "Epochs:  284 | Loss: 36.4138933671\n",
            "Epochs:  285 | Loss: 35.7695973997\n",
            "Epochs:  286 | Loss: 35.9514011346\n",
            "Epochs:  287 | Loss: 36.1279216974\n",
            "Epochs:  288 | Loss: 36.3535159096\n",
            "Epochs:  289 | Loss: 35.9034074140\n",
            "Epochs:  290 | Loss: 35.5740009569\n",
            "Epochs:  291 | Loss: 35.5694785933\n",
            "Epochs:  292 | Loss: 36.0143185407\n",
            "Epochs:  293 | Loss: 35.8530648099\n",
            "Epochs:  294 | Loss: 35.5428219189\n",
            "Epochs:  295 | Loss: 35.5208605707\n",
            "Epochs:  296 | Loss: 36.0979495248\n",
            "Epochs:  297 | Loss: 35.6953280477\n",
            "Epochs:  298 | Loss: 35.6861075642\n",
            "Epochs:  299 | Loss: 35.2978207409\n",
            "Epochs:  300 | Loss: 35.2718593519\n",
            "Epochs:  301 | Loss: 35.0002354355\n",
            "Epochs:  302 | Loss: 35.4283801741\n",
            "Epochs:  303 | Loss: 35.8007698302\n",
            "Epochs:  304 | Loss: 36.0736666051\n",
            "Epochs:  305 | Loss: 35.8763574480\n",
            "Epochs:  306 | Loss: 34.8293688717\n",
            "Epochs:  307 | Loss: 34.6435467527\n",
            "Epochs:  308 | Loss: 36.1948849782\n",
            "Epochs:  309 | Loss: 35.2299057580\n",
            "Epochs:  310 | Loss: 35.5558696673\n",
            "Epochs:  311 | Loss: 35.6930810203\n",
            "Epochs:  312 | Loss: 36.5370493875\n",
            "Epochs:  313 | Loss: 35.3294041789\n",
            "Epochs:  314 | Loss: 35.1020891774\n",
            "Epochs:  315 | Loss: 35.1608966293\n",
            "Epochs:  316 | Loss: 35.3217359875\n",
            "Epochs:  317 | Loss: 35.2681356668\n",
            "Epochs:  318 | Loss: 34.6162335776\n",
            "Epochs:  319 | Loss: 34.2672733459\n",
            "Epochs:  320 | Loss: 35.4996981526\n",
            "Epochs:  321 | Loss: 34.8940438539\n",
            "Epochs:  322 | Loss: 35.1003116171\n",
            "Epochs:  323 | Loss: 35.4371984568\n",
            "Epochs:  324 | Loss: 35.2532157870\n",
            "Epochs:  325 | Loss: 34.8472048806\n",
            "Epochs:  326 | Loss: 35.4269639405\n",
            "Epochs:  327 | Loss: 35.8416224980\n",
            "Epochs:  328 | Loss: 34.8010063381\n",
            "Epochs:  329 | Loss: 34.8660441959\n",
            "Epochs:  330 | Loss: 35.6025608001\n",
            "Epochs:  331 | Loss: 34.7797749622\n",
            "Epochs:  332 | Loss: 35.4390594219\n",
            "Epochs:  333 | Loss: 35.2051844869\n",
            "Epochs:  334 | Loss: 35.0641323257\n",
            "Epochs:  335 | Loss: 35.4413137089\n",
            "Epochs:  336 | Loss: 35.4545411779\n",
            "Epochs:  337 | Loss: 34.5563445623\n",
            "Epochs:  338 | Loss: 35.2581014291\n",
            "Epochs:  339 | Loss: 34.9716090878\n",
            "Epochs:  340 | Loss: 34.8622186949\n",
            "Epochs:  341 | Loss: 34.1479678977\n",
            "Epochs:  342 | Loss: 34.5204639990\n",
            "Epochs:  343 | Loss: 34.9250346447\n",
            "Epochs:  344 | Loss: 34.6089581445\n",
            "Epochs:  345 | Loss: 34.3850752719\n",
            "Epochs:  346 | Loss: 34.7616907426\n",
            "Epochs:  347 | Loss: 35.0286349596\n",
            "Epochs:  348 | Loss: 34.2169351171\n",
            "Epochs:  349 | Loss: 34.9063786769\n",
            "Epochs:  350 | Loss: 35.0506193509\n",
            "Epochs:  351 | Loss: 34.3023252544\n",
            "Epochs:  352 | Loss: 35.3329315169\n",
            "Epochs:  353 | Loss: 34.4956186840\n",
            "Epochs:  354 | Loss: 35.1079656583\n",
            "Epochs:  355 | Loss: 35.0800146466\n",
            "Epochs:  356 | Loss: 35.5742159771\n",
            "Epochs:  357 | Loss: 34.3960964759\n",
            "Epochs:  358 | Loss: 34.4612727346\n",
            "Epochs:  359 | Loss: 34.4771329277\n",
            "Epochs:  360 | Loss: 34.2170874030\n",
            "Epochs:  361 | Loss: 35.3091680254\n",
            "Epochs:  362 | Loss: 33.6631857042\n",
            "Epochs:  363 | Loss: 33.6835958650\n",
            "Epochs:  364 | Loss: 34.1711925555\n",
            "Epochs:  365 | Loss: 34.3759525637\n",
            "Epochs:  366 | Loss: 34.8082685016\n",
            "Epochs:  367 | Loss: 35.7177959256\n",
            "Epochs:  368 | Loss: 34.1344832908\n",
            "Epochs:  369 | Loss: 34.2052115526\n",
            "Epochs:  370 | Loss: 33.8316523290\n",
            "Epochs:  371 | Loss: 34.2908175373\n",
            "Epochs:  372 | Loss: 34.1596513492\n",
            "Epochs:  373 | Loss: 34.3647188540\n",
            "Epochs:  374 | Loss: 33.5963553606\n",
            "Epochs:  375 | Loss: 33.8609979938\n",
            "Epochs:  376 | Loss: 34.1148141204\n",
            "Epochs:  377 | Loss: 34.6057176725\n",
            "Epochs:  378 | Loss: 34.5541302942\n",
            "Epochs:  379 | Loss: 34.3442157719\n",
            "Epochs:  380 | Loss: 34.8112798983\n",
            "Epochs:  381 | Loss: 34.0188022879\n",
            "Epochs:  382 | Loss: 34.1208335883\n",
            "Epochs:  383 | Loss: 33.9623208701\n",
            "Epochs:  384 | Loss: 33.0234423362\n",
            "Epochs:  385 | Loss: 34.1779878042\n",
            "Epochs:  386 | Loss: 33.8499374951\n",
            "Epochs:  387 | Loss: 34.4369162026\n",
            "Epochs:  388 | Loss: 34.6074090979\n",
            "Epochs:  389 | Loss: 34.3020344847\n",
            "Epochs:  390 | Loss: 33.9026082296\n",
            "Epochs:  391 | Loss: 33.8072885619\n",
            "Epochs:  392 | Loss: 33.5605659110\n",
            "Epochs:  393 | Loss: 34.2555655641\n",
            "Epochs:  394 | Loss: 33.5431294357\n",
            "Epochs:  395 | Loss: 34.0285520078\n",
            "Epochs:  396 | Loss: 34.2117062560\n",
            "Epochs:  397 | Loss: 33.3431190584\n",
            "Epochs:  398 | Loss: 33.8246057938\n",
            "Epochs:  399 | Loss: 34.2067359806\n",
            "Epochs:  400 | Loss: 33.2112711190\n",
            "Epochs:  401 | Loss: 34.2788321555\n",
            "Epochs:  402 | Loss: 34.0458160625\n",
            "Epochs:  403 | Loss: 33.1181981653\n",
            "Epochs:  404 | Loss: 34.1335656376\n",
            "Epochs:  405 | Loss: 34.7354460831\n",
            "Epochs:  406 | Loss: 34.3071193699\n",
            "Epochs:  407 | Loss: 34.1241997141\n",
            "Epochs:  408 | Loss: 33.7005536502\n",
            "Epochs:  409 | Loss: 34.4628299627\n",
            "Epochs:  410 | Loss: 34.1623952975\n",
            "Epochs:  411 | Loss: 33.6870405650\n",
            "Epochs:  412 | Loss: 34.1722593997\n",
            "Epochs:  413 | Loss: 34.2548768375\n",
            "Epochs:  414 | Loss: 34.2410687050\n",
            "Epochs:  415 | Loss: 34.0856262537\n",
            "Epochs:  416 | Loss: 34.1673132210\n",
            "Epochs:  417 | Loss: 33.3462795156\n",
            "Epochs:  418 | Loss: 33.1825009551\n",
            "Epochs:  419 | Loss: 34.4897185953\n",
            "Epochs:  420 | Loss: 33.3454918484\n",
            "Epochs:  421 | Loss: 34.2530572484\n",
            "Epochs:  422 | Loss: 34.4589435254\n",
            "Epochs:  423 | Loss: 32.8743396822\n",
            "Epochs:  424 | Loss: 33.7137478667\n",
            "Epochs:  425 | Loss: 34.4846902359\n",
            "Epochs:  426 | Loss: 32.7659315443\n",
            "Epochs:  427 | Loss: 33.3902152280\n",
            "Epochs:  428 | Loss: 34.2592175873\n",
            "Epochs:  429 | Loss: 34.0900733732\n",
            "Epochs:  430 | Loss: 33.6698108579\n",
            "Epochs:  431 | Loss: 33.3762253862\n",
            "Epochs:  432 | Loss: 33.6968188427\n",
            "Epochs:  433 | Loss: 33.7577683063\n",
            "Epochs:  434 | Loss: 33.9108186092\n",
            "Epochs:  435 | Loss: 33.8854329154\n",
            "Epochs:  436 | Loss: 34.3267434834\n",
            "Epochs:  437 | Loss: 33.2566245243\n",
            "Epochs:  438 | Loss: 33.7544968360\n",
            "Epochs:  439 | Loss: 33.8298998667\n",
            "Epochs:  440 | Loss: 33.9097534225\n",
            "Epochs:  441 | Loss: 34.4185582699\n",
            "Epochs:  442 | Loss: 33.3068606048\n",
            "Epochs:  443 | Loss: 33.3163608636\n",
            "Epochs:  444 | Loss: 33.4534408382\n",
            "Epochs:  445 | Loss: 33.4471159413\n",
            "Epochs:  446 | Loss: 33.8582493718\n",
            "Epochs:  447 | Loss: 33.6282736075\n",
            "Epochs:  448 | Loss: 33.3334604644\n",
            "Epochs:  449 | Loss: 34.4308881258\n",
            "Epochs:  450 | Loss: 34.1552258010\n",
            "Epochs:  451 | Loss: 34.2601341790\n",
            "Epochs:  452 | Loss: 33.1997674415\n",
            "Epochs:  453 | Loss: 33.1305179927\n",
            "Epochs:  454 | Loss: 32.8412624449\n",
            "Epochs:  455 | Loss: 33.3374512413\n",
            "Epochs:  456 | Loss: 33.5373938540\n",
            "Epochs:  457 | Loss: 33.2097884716\n",
            "Epochs:  458 | Loss: 32.8358701006\n",
            "Epochs:  459 | Loss: 34.5432977652\n",
            "Epochs:  460 | Loss: 33.1443047570\n",
            "Epochs:  461 | Loss: 33.3266847952\n",
            "Epochs:  462 | Loss: 32.7250769495\n",
            "Epochs:  463 | Loss: 33.0316825890\n",
            "Epochs:  464 | Loss: 32.8064753487\n",
            "Epochs:  465 | Loss: 33.1129233649\n",
            "Epochs:  466 | Loss: 33.0318385847\n",
            "Epochs:  467 | Loss: 33.6410346661\n",
            "Epochs:  468 | Loss: 33.1836216712\n",
            "Epochs:  469 | Loss: 34.2679508147\n",
            "Epochs:  470 | Loss: 33.7695594814\n",
            "Epochs:  471 | Loss: 33.0740959889\n",
            "Epochs:  472 | Loss: 33.8966462953\n",
            "Epochs:  473 | Loss: 34.1779820688\n",
            "Epochs:  474 | Loss: 33.6794974951\n",
            "Epochs:  475 | Loss: 33.4178668742\n",
            "Epochs:  476 | Loss: 33.1417806005\n",
            "Epochs:  477 | Loss: 32.9866915289\n",
            "Epochs:  478 | Loss: 33.7023326032\n",
            "Epochs:  479 | Loss: 33.1697639956\n",
            "Epochs:  480 | Loss: 32.9740774237\n",
            "Epochs:  481 | Loss: 33.5477620104\n",
            "Epochs:  482 | Loss: 33.5949579511\n",
            "Epochs:  483 | Loss: 33.4754892169\n",
            "Epochs:  484 | Loss: 32.6956278557\n",
            "Epochs:  485 | Loss: 33.5617461399\n",
            "Epochs:  486 | Loss: 32.6783805033\n",
            "Epochs:  487 | Loss: 33.8324577343\n",
            "Epochs:  488 | Loss: 34.2786940239\n",
            "Epochs:  489 | Loss: 33.5369370736\n",
            "Epochs:  490 | Loss: 33.7678521738\n",
            "Epochs:  491 | Loss: 32.8840787212\n",
            "Epochs:  492 | Loss: 32.6429715175\n",
            "Epochs:  493 | Loss: 32.4629736244\n",
            "Epochs:  494 | Loss: 33.0749934914\n",
            "Epochs:  495 | Loss: 33.4028391666\n",
            "Epochs:  496 | Loss: 33.6317950411\n",
            "Epochs:  497 | Loss: 33.1609095187\n",
            "Epochs:  498 | Loss: 32.8616700893\n",
            "Epochs:  499 | Loss: 33.6147439221\n",
            "Epochs:  500 | Loss: 32.2850865244\n"
          ]
        }
      ],
      "source": [
        "# Train the model \n",
        "epochs = 500\n",
        "epochresults = []\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "    \n",
        "    #---load data into GPU----\n",
        "    inputs = inputs.to('cuda')\n",
        "    labels = labels.to('cuda')\n",
        "    #-------------------------\n",
        "    \n",
        "    outputs = model(inputs)  # forward propagation\n",
        "    #print(outputs)\n",
        "    loss = loss_fn(outputs, labels)    \n",
        "    # to remove previous epoch gradients\n",
        "    optimizer.zero_grad()    # set optimizer to zero grad\n",
        "    loss.backward()    \n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()  \n",
        "\n",
        "  epoch_loss = running_loss / len(train_dataloader)\n",
        "  epochresults.append(epoch_loss)\n",
        "  print(f'Epochs:{epoch + 1:5d} | ' \\\n",
        "        f'Loss: {epoch_loss:.10f}')\n",
        "\n",
        "#something is wrong when increasing epoch 100->1000 the time for each 100 epochs goes up xD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "TQjW4ZbJyyLc",
        "outputId": "c253443d-be9f-461f-d212-9642a8c87ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.set(xlabel='epoch', ylabel='loss', title=\"Loss function\")\n",
        "\n",
        "plt.plot(epochresults, 'red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ejqMUbqkyyLc",
        "outputId": "b596512f-2c0a-4f65-c78e-160f516b7dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set loss: 24.7904\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient tracking\n",
        "    for inputs, labels in test_dataloader:\n",
        "\n",
        "        #---load data into GPU----\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        #-------------------------\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        test_loss += loss_fn(outputs, labels).item()\n",
        "\n",
        "test_loss /= len(test_dataloader)\n",
        "print(f'Test set loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "0t3X8JCRyyLd"
      },
      "outputs": [],
      "source": [
        "# # save the trained model\n",
        "# PATH = 'model3.pth'\n",
        "# torch.save(model.state_dict(), PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}